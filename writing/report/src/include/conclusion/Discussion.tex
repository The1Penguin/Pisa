\section{Discussion \& Future work}\label{sec:discussion}

\paragraph*{Relevance} was measured in \cref{chp:results} using precision and recall analysis.
The precision gathered shows that precision was around $80\%-90\%$ for the different cases.
This means that when Pisa generates conjectures, most of them will be a known lemma.
When it comes to recall, it varied from $45\%-80\%$.
Pisa is therefore not able to find all the potential conjectures, using the default settings.
Increasing the size term when call Pisa could help with this, but what is more likely the impacting factor is QuickSpec's pruning.
For example, \cref{succ_add} wasn't found by Pisa.
However, it could be derived from \cref{add_succ} and \cref{nat_comm}.
This means that the measurements taken here are a worst case scenario for this metric, but it still performed well in the polymorphic case.
Further, some are defintionally equal, such as \cref{nat_succ,nat_plus_one} due to $n+1 = §succ§\ n$, but are split in mathlib.

\vspace{-0.7cm}
\begin{align}
\forall n\ m : \mathbb{N}.\ (§succ§\ n) + m &= §succ§\ (n + m) \label{succ_add} \\
\forall n\ m : \mathbb{N}.\ n + §succ§\ m &= §succ§\ (n + m) \label{add_succ} \\
\forall n\ m : \mathbb{N}.\ n + m &= m + n \label{nat_comm} \\
\forall n\ m : \mathbb{N}.\ (§succ§\ n) * m &= (n * m) + m \label{nat_succ} \\
\forall n\ m : \mathbb{N}.\ (n + 1) * m &= (n * m) + m \label{nat_plus_one}
\end{align}

\paragraph*{Performance} is an aspect that could have been evaluated due to the approach taken.
The time for finding conjectures is an aspect of the work that influences the usability of the tool.
We instead took the approach that it was more important to provide conjectures at some point, rather than to be fast.
The reason for this is that utilizing an ITP will inherently have a slower type checking and compile time, therefore user are not as bothered by tooling taking a bit longer to execute.

\paragraph*{Instantiations of polymorphic data types} is a limitation with the current implementation.
The implementation is not able to create an alias for these types, and therefore the ability to reason about these types are lost.
This is a consequence of the monomorphic conversions to and from values and data types described in \cref{sec:pisa-haskell:translation}.
By instead constructing a type class for this conversion, Haskell could resolve these instantiations.
However, \haskell{Poly} as a type would still be needed for the general cases when one wants to check polymorphic functions.

\paragraph*{An auto proof engine} as the final step was considered.
However, this was not implemented for two main reasons, one being time constraints, the other being how the usage of auto proof engines such as ``Aesop''~\autocite{Aesop}, or \citetitle{LeanAuto}~\autocite{LeanAuto} work.
The issue with how they work is that they usually require more user interaction than other auto proof engines for other languages.
This can be in the user saying which lemmas are available, or how it can do case splits.
This addition makes it harder to use with auto generated conjectures.

\paragraph*{Lean's syntax is extensible} as mentioned in \cref{sec:lean:extensible-syntax}.
It is therefore hard to define where the base syntax ends and extended begins.
Because of this, attempting to parse the source code is not applicable, without first resolving the extensions.
An attempt at using \citetitle{ast-export} by~\cite{ast-export} was done, but this approach was deemed too complicated because of the nature of the AST.
Further, attempting to fetch transient dependencies based on the AST would be more difficult, if the extensions have not been resolved.

\subsection{Alternative approaches}\label{sec:discussion:alternative-approaches}
During the project, we saw multiple approaches we could have taken, and implemented some of them to a point.

The translation approach that was initially used for the project proved difficult to implement.
It attempted to do a faithful translation from Lean to Haskell syntax.
The issue that arose was the utilization of dependent types throughout the AST of the lambda calculus generated by the exporter.
For example, when case splitting on a boolean, it will utilize the §recursor§, which uses dependent types.
This made this approach unfeasible to implement, without hacking in edge cases.
An idea that we thought of was to try to evaluate these statements when encountered, to find out what it would resolve to, since they should be fully satisfied.
This was however not implemented.

Another approach that we tried was creating an interpreter for the AST of the lambda calculus itself.
Like the pure translation, it hit snags as well.
These snags were usually related to the §recursor§, which would be called in many different ways.
Dependent types could theoretically be encoded using lambda expressions, since it is kept in the AST, however we believe it would have been difficult to auto generate arguments that keep these invariants with QuickCheck.
This is the reason that the current implementation separates the values from the IR when generating them.

The approach that we did not attempt, but think is the most reasonable now is to reimplement QuickSpec or RoughSpec in Lean.
With that, one could build up macros to utilize a tool in a similar way to Pisa.
At the beginning of the project, we did not know of ``Plausible''~\autocite{Plausible}, which fits the same part as QuickCheck does for QuickSpec and RoughSpec.
Based on Plausible's source code, it is heavily inspired by QuickCheck.
This would avoid some issues, such as a translating the definitions into a different type system which doesn't have the same capabilities as Lean.
However, one thing that might happen is that certain issues may arise due to dependent types in the reimplementation, similarly to the full translation approach.
Therefore, we would recommend limiting the scope by utilizing a similar template system as RoughSpec, just to make something work.
